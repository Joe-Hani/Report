\documentclass{jsarticle}
\usepackage{amsmath,amssymb,mathrsfs}
\usepackage[all]{xy}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{dfn}{定義}[section]
\newtheorem{thm}{定理}[section]
\newtheorem{lem}{補題}[section]
\newtheorem{pro}{命題}[section]
\newtheorem{cor}{系}[section]
\newtheorem{ex}{例}[section]
\begin{document}
\title{$k$平均法}
\date{}
\maketitle
$X=\{\mathbf{x}_{1},\dots,\mathbf{x}_{n}\}$を$\mathbb{R}^{m}$の部分集合とし、$k$を自然数とする。
以下のアルゴリズムを用いて、$X$を$k$個のクラスターへと分類する。このアルゴリズムを\textbf{$k$平均法}という。
\begin{enumerate}
\item[(1)] $k$個のデータ$\mathbf{x}_{i_{1}},\dots,\mathbf{x}_{i_{k}}\in X$を取る。
\item[(2)] 任意のデータ$\mathbf{x}\in X$に対して、$\mathbf{x}_{i_{1}},\dots,\mathbf{x}_{i_{k}}$の中で
$\mathbf{x}$に最も距離が近い$\mathbf{x}_{i_{j}}$（$1\leq j\leq k$）を取る。
\item[(3)] $C(x_{i_{j}})=\{\mathbf{x}\in X\mid\text{$\mathbf{x}_{i_{j}}$は$\mathbf{x}_{i_{1}},\dots,\mathbf{x}_{i_{k}}$の中で$\mathbf{x}$に最も距離が近い}\}$とおき、$x_{i_{j}}$を$\frac{1}{|C(x_{i_{j}})|}\sum_{\mathbf{x}\in C(x_{i_{j}})}\mathbf{x}$で置き換える（ただし、$|C(x_{i_{j}})|$は$C(x_{i_{j}})$の元の個数を表す）。
\item[(4)] (1)に戻る。
\end{enumerate}
\begin{ex} (コードは$k$平均法.ipynb) 
\begin{verbatim}
#データを生成
import numpy as np
X = np.array([[-2.7], [-1.3], [0.7], [3.5], [5.1]])

from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=2) #k=2として、k平均法を適用
y_pred = kmeans.fit(X).predict(X) #Xをクラスタリングする
print(y_pred)
>[0 0 0 1 1]
\end{verbatim}
\end{ex}
\begin{thebibliography}{99}
\bibitem{B} Andriy Burkov. (2019). The hundred-page machine learning book.
\bibitem{DFO} Marc Peter Deisenroth., A. Aldo Faisal., Cheng Soon Ong. (2020). Mathematics for machine learning. Cambridge University Press.
\bibitem{G} Aur\"{e}lien G\"{e}ron. (2019). Hands-on machine learning with Scikit-Learn, Keras \&  TensorFlow. 2nd Edition. Oreilly.
\bibitem{OSMW} 小縣信也., 斎藤翔汰., 溝口聡., 若杉一幸. (2021). ディープラーニングE資格エンジニア問題集 インプレス.
\bibitem{RM} Sebastian Raschka., Vahid Mirjalili. (2019). Python machine learning. Third Edition. Packt.

\end{thebibliography}
\end{document}
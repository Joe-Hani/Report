\documentclass{jsarticle}
\usepackage{amsmath,amssymb,mathrsfs}
\usepackage[all]{xy}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{dfn}{定義}[section]
\newtheorem{thm}{定理}[section]
\newtheorem{lem}{補題}[section]
\newtheorem{pro}{命題}[section]
\newtheorem{cor}{系}[section]
\newtheorem{ex}{例}[section]
\begin{document}
\title{サポートベクトルマシーン}
\date{}
\maketitle
$\{(\mathbf{x}_{i},y_{i})\}_{i=1}^{N}$をラベル付けされたデータとし、$N$をデータの数、$\mathbf{x}_{i}$を$D$次元特徴ベクトル、$y_{i}$を$\mathbf{x}_{i}$のラベルとする。ただし、$y_{i}$の値は$-1,1$のいずれかであるとする。
$\mathbf{w}$を$D$次元ベクトル、$b$を実数とし、
\begin{equation*}
f_{\mathbf{w},b}(\mathbf{x}):=\mathbf{w}\mathbf{x}+b
\end{equation*}
とおく。この式を用いて、未知の$D$次元特徴ベクトル$\mathbf{x}$に対して、ラベル$y=f_{\mathbf{w},b}(\mathbf{x})$を予測する。すなわち、$f_{\mathbf{w},b}$が以下の条件
\begin{enumerate}
\item[(1)] $y_{i}=1$ならば、$f_{\mathbf{w},b}(\mathbf{x}_{i})\geq 0$
\item[(2)] $y_{i}=-1$ならば、$f_{\mathbf{w},b}(\mathbf{x}_{i})< 0$
\end{enumerate}
を満たすような$\mathbf{w},b$の値を求める。上の条件は
\begin{equation}
y_{i}f_{\mathbf{w},b}(\mathbf{x}_{i})\geq 0
\end{equation}
という条件にまとめることができる。$f_{\mathbf{w},b}(\mathbf{x})=0$を\textbf{分類境界}という。$f_{\mathbf{w},b}(\mathbf{x})=0$は自身を境界線として、$X$を二つのグループ
\begin{equation*}
A=\{\mathbf{x}\in X\mid f_{\mathbf{w},b}(\mathbf{x}_{i})\geq 0\},B=\{\mathbf{x}\in X\mid f_{\mathbf{w},b}(\mathbf{x}_{i})< 0\}
\end{equation*}
へと分類する。このように、$X$を$f_{\mathbf{w},b}(\mathbf{x})=0$によって、二つのグループへと分類することを\textbf{ハードマージン}という。分類境界$f_{\mathbf{w},b}(\mathbf{x})=0$と分類境界から最も近くにあるデータ$\mathbf{x}_{i}\in X$との距離は$|f_{\mathbf{w},b}(\mathbf{x}_{i})|/\|\mathbf{w}\|$である。(1)より、
$|f_{\mathbf{w},b}(\mathbf{x}_{i})|/\|\mathbf{w}\|=y_{i}f_{\mathbf{w},b}(\mathbf{x}_{i})/\|\mathbf{w}\|$が成り立つ。
\begin{equation*}
M(\mathbf{w},b):=\min\{y_{i}f_{\mathbf{w},b}(\mathbf{x}_{i})\mid 1\leq i\leq N\}
\end{equation*}
とおく。(1)を満たす$\mathbf{w},b$の中で、$M(\mathbf{w},b)/\|\mathbf{w}\|$を最大化するものを求める。
すなわち、最適化問題
\begin{equation}
\max_{\mathbf{w},b}M(\mathbf{w},b)/\|\mathbf{w}\|,\quad \text{ただし、全ての$1\leq i\leq N$に対して、$M(\mathbf{w},b)\leq y_{i}f_{\mathbf{w},b}(\mathbf{x}_{i})$}
\end{equation}
を解く。これは、分類境界$f_{\mathbf{w},b}(\mathbf{x})=0$と分類境界からデータへの距離を最大化することを意味する。(2)は以下の扱いやすい形
\begin{equation}
\min_{\mathbf{w},b}\|\mathbf{w}\|^{2}/2,\quad \text{ただし、全ての$1\leq i\leq N$に対して、$y_{i}f_{\mathbf{w},b}(\mathbf{x}_{i})\geq 1$}
\end{equation}
へと変形できる。ハードマージンでは、$X$を二つのグループへと分類する分類境界$f_{\mathbf{w},b}(\mathbf{x})=0$が存在することを仮定した。しかし、現実の多くの問題においては、そのような仮定は強すぎる。
以下では、この仮定をなくし、必ずしも二つのグループへと分類できないようなデータへ適用できるように拡張する。
これを行うために、$\xi=(\xi_{1},\dots,\xi_{N})$（$\xi_{i}\geq 0$）とおき、$C$を正の実数とし、(3)を一般化して、
最適化問題
\begin{equation}
\min_{\mathbf{w},b,\xi}(\|\mathbf{w}\|^{2}/2+C\sum_{i=1}^{N}\xi_{i}),\quad \text{ただし、全ての$1\leq i\leq N$に対して、$y_{i}f_{\mathbf{w},b}(\mathbf{x}_{i})\geq 1-\xi_{i}$}
\end{equation}
を解く。このように、$X$が必ずしも二つのグループへと分類できないとし、分類誤差を許容した上で
二つのグループへと分類することを\textbf{ソフトマージン}という。$C\rightarrow\infty$であるとき、
(4)は(3)に一致する。逆に、$C\rightarrow 0$であるときは、誤分類が許容されやすくなる。実装する際は適切な$C$を設定する必要がある。
\begin{ex} (コードはサポートベクトルマシーン.ipynb) irisデータセットを用いてサポートベクトルマシーンを実装する。
\begin{verbatim}
#データを取得
import numpy as np
from sklearn import datasets
iris = datasets.load_iris()
X = iris["data"][:, (2, 3)]  # petal length, petal width
y = (iris["target"] == 2).astype(np.float64)  # Iris virginica

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC
svm_clf = Pipeline([
        ("scaler", StandardScaler()),
        ("linear_svc", LinearSVC(C=1, loss="hinge", random_state=42)),
    ])

svm_clf.fit(X, y) #データを正規化し、C=1としてサポートベクトルマシーンを適用

svm_clf.predict([[5.5, 1.7]]) #値を予測
>array([1.])
\end{verbatim}
\end{ex}
\begin{thebibliography}{99}
\bibitem{B} Andriy Burkov. (2019). The hundred-page machine learning book.
\bibitem{DFO} Marc Peter Deisenroth., A. Aldo Faisal., Cheng Soon Ong. (2020). Mathematics for machine learning. Cambridge University Press.
\bibitem{G} Aur\"{e}lien G\"{e}ron. (2019). Hands-on machine learning with Scikit-Learn, Keras \&  TensorFlow. 2nd Edition. Oreilly.
\bibitem{OSMW} 小縣信也., 斎藤翔汰., 溝口聡., 若杉一幸. (2021). ディープラーニングE資格エンジニア問題集 インプレス.
\bibitem{RM} Sebastian Raschka., Vahid Mirjalili. (2019). Python machine learning. Third Edition. Packt.

\end{thebibliography}
\end{document}